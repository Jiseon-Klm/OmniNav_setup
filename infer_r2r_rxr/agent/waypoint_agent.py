flow_match = False
import json
import numpy as np

try:
    from habitat import Env
    from habitat.core.agent import Agent
    import imageio
    from habitat.utils.visualizations import maps
except:
    pass

# imageioÎäî Î≥ÑÎèÑÎ°ú import (GIF Ï†ÄÏû•Ïö©)
try:
    import imageio
except:
    imageio = None
from tqdm import trange
import os
import torch
import cv2
import time
from scipy.spatial.transform import Rotation as R
from safetensors.torch import load_file

from PIL import Image
from copy import deepcopy

from transformers import AutoProcessor, AutoTokenizer, AutoConfig, Qwen2VLForConditionalGeneration, \
    Qwen2_5_VLForConditionalGeneration, AutoModelForImageTextToText
from qwen_vl_utils import process_vision_info

SAVE_RANDER_IMG = True
PREDICT_SCALE = 0.3  # Original scale (model outputs normalized coords)
MAX_HISTORY_FRAMES = 15  # Slightly less than paper (20) for 16GB VRAM
magic_crop = False
NUM_CURRENT_IMAGE = 3
INPUT_IMG_SIZE = (480, 426)  # ÎÖºÎ¨∏ Í∏∞Ï§Ä Ìï¥ÏÉÅÎèÑ (480x426)
HISTORY_RESIZE_RATIO = 0.25  # History ~96x85

MODEL_TYPE = 'Waypoint'
NUM_ACTION_TRUNK = 5
NUM_EXCUTE_ACTION_IN_TRUNK = 1


def get_model_name_from_path(model_path):
    return '/'.join(model_path.split('/')[-3:])


def evaluate_agent(config, split_id, dataset, model_path, result_path) -> None:
    env = Env(config.TASK_CONFIG, dataset)

    model_name = get_model_name_from_path(model_path)
    result_path = os.path.join(result_path, model_name)

    agent = Waypoint_Agent(model_path, result_path)

    num_episodes = len(env.episodes)

    EARLY_STOP_ROTATION = config.EVAL.EARLY_STOP_ROTATION
    EARLY_STOP_STEPS = config.EVAL.EARLY_STOP_STEPS

    target_key = {"distance_to_goal", "success", "spl", "path_length", "oracle_success"}

    count = 0

    for _ in trange(num_episodes, desc=config.EVAL.IDENTIFICATION + "-{}".format(split_id)):
        obs = env.reset()
        iter_step = 0
        agent.reset()

        continuse_rotation_count = 0
        continuse_collision_count = 0
        last_dtg = 999
        while not env.episode_over:

            info = env.get_metrics()

            if info["distance_to_goal"] != last_dtg:
                last_dtg = info["distance_to_goal"]
                continuse_rotation_count = 0
                continuse_collision_count = 0
            else:
                continuse_rotation_count += 1
            obs['pose'] = {'position': env._sim.get_agent_state().position.tolist(),
                           'rotation': [env._sim.get_agent_state().rotation.w,
                                        env._sim.get_agent_state().rotation.x,
                                        env._sim.get_agent_state().rotation.y,
                                        env._sim.get_agent_state().rotation.z]}
            with torch.no_grad():
                action = agent.act(obs, info, env.current_episode.episode_id)
            if action[
                'arrive_pred'] > 0:  # or np.max(np.linalg.norm(action['action'],axis=1)) < 0.2:
                action = {"action": "STOP"}
            elif action[
                'arrive_pred'] >= 0.5:  # or np.max(np.linalg.norm(action['action'],axis=1)) < 0.2:
                action = {"action": "STOP"}
            else:
                select_way_point_idx = 0
                print(action['action'])
                print(action['recover_angle'])
                way_point_loc = action['action'][select_way_point_idx, :]
                recover_angle = action['recover_angle'][select_way_point_idx]
                print("way_point_loc", "recover_angle")
                print(way_point_loc)
                print(recover_angle)
                distance = np.linalg.norm(way_point_loc)
                print(distance)
                theta = np.arctan2(-way_point_loc[0], way_point_loc[1])

                action = {"action": "GO_TOWARD_POINT", "action_args": {"theta": recover_angle, "r": distance}}
                print(f'step: {iter_step}, action: {action}')

            if continuse_rotation_count > EARLY_STOP_ROTATION or iter_step > EARLY_STOP_STEPS:
                action = {"action": "STOP"}
            obs = env.step(action)
            iter_step += 1
        info = env.get_metrics()
        result_dict = {k: info[k] for k in target_key if k in info}
        result_dict["id"] = env.current_episode.episode_id
        count += 1

        with open(
                os.path.join(os.path.join(result_path, "log"), "stats_{}.json".format(env.current_episode.episode_id)),
                "w") as f:
            json.dump(result_dict, f, indent=4)


class QwenModel():
    def __init__(self, model_path):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.processor = AutoProcessor.from_pretrained(model_path)
        self.nav_version = 'special_token'
        # Î°úÎî© ÏÜçÎèÑ Ìñ•ÏÉÅÏùÑ ÏúÑÌïú ÏÑ§Ï†ï
        import os
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # ÏñëÏûêÌôî ÏóÜÏù¥ ÏõêÎ≥∏ Î™®Îç∏ Î°úÎìú (ÎÖºÎ¨∏Í≥º ÎèôÏùº)
        # GB10 (sm_121a)Îäî Îß§Ïö∞ ÏµúÏã† GPUÎùº flash_attention_2/triton Ìò∏ÌôòÏÑ± Î¨∏Ï†ú ÏûàÏùå
        # ÏïàÏ†ïÏÑ±ÏùÑ ÏúÑÌï¥ eager mode ÏÇ¨Ïö©
        # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî: device_map Î™ÖÏãúÏ†Å ÏßÄÏ†ï Ïãú max_memoryÎäî Î¨¥ÏãúÎêòÎØÄÎ°ú Ï†úÍ±∞
        # low_cpu_mem_usageÎ°ú Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ Î°úÎî©Îßå ÌôúÏÑ±Ìôî
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map={"": "cuda:0"},
            attn_implementation="eager",  # GB10 Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥ eager ÏÇ¨Ïö©
            low_cpu_mem_usage=True  # Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ Î°úÎî©
        )
        print("‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å (eager mode, Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî)")
        
        # torch.compileÍ≥º triton Í¥ÄÎ†® ÏµúÏ†ÅÌôîÎäî GB10ÏóêÏÑú Ìò∏ÌôòÏÑ± Î¨∏Ï†úÎ°ú Ï†úÏô∏
        # torch.compileÎèÑ tritonÏùÑ ÏÇ¨Ïö©ÌïòÎØÄÎ°ú GB10ÏóêÏÑú ÏûëÎèôÌïòÏßÄ ÏïäÏùÑ Ïàò ÏûàÏùå
        
        # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî: eval mode + Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÎπÑÌôúÏÑ±Ìôî
        self.model.eval()
        # Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïùò requires_gradÎ•º FalseÎ°ú ÏÑ§Ï†ï (ÏïàÏ†ÑÏû•Ïπò)
        for param in self.model.parameters():
            param.requires_grad = False
        
        # CUDA Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (Ï¥àÍ∏∞ Î°úÎî© ÌõÑ)
        torch.cuda.empty_cache()
        
        # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï∂úÎ†• (ÎîîÎ≤ÑÍπÖÏö©)
        if torch.cuda.is_available():
            mem_allocated = torch.cuda.memory_allocated(0) / 1024**3  # GB
            mem_reserved = torch.cuda.memory_reserved(0) / 1024**3  # GB
            print(f"üìä GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: {mem_allocated:.2f}GB Ìï†ÎãπÎê® / {mem_reserved:.2f}GB ÏòàÏïΩÎê®")
        
        # KV cache Ï¥àÍ∏∞Ìôî (Ìñ•ÌõÑ Íµ¨ÌòÑÏö©)
        self.past_key_values = None
        self.use_kv_cache = False  # forward()ÏóêÏÑúÎäî ÏßÄÏõê Ïïà Ìï®
        # Note: from_pretrained already loads safetensors, don't override with load_state_dict
        # which can corrupt 8-bit quantized weights

    @staticmethod
    def qwen_data_pack(images, user_content):
        content = []
        for idx, image in enumerate(images):
            if idx >= len(images) - NUM_CURRENT_IMAGE:
                cur_json = {
                    "type": "image",
                    "image": image,
                    "resized_height": INPUT_IMG_SIZE[1],
                    "resized_width": INPUT_IMG_SIZE[0],
                }
            else:
                cur_json = {
                    "type": "image",
                    "image": image,
                    "resized_height": INPUT_IMG_SIZE[1] * HISTORY_RESIZE_RATIO,
                    "resized_width": INPUT_IMG_SIZE[0] * HISTORY_RESIZE_RATIO,
                }
            content.append(cur_json)
        content.append({
            "type": "text",
            "text": user_content,
        })
        messages = [
            {
                "role": "user",
                "content": content
            },
        ]
        return messages

    def qwen_infer(self, messages, use_past_key_values=None):
        """
        Args:
            messages: ÏûÖÎ†• Î©îÏãúÏßÄ
            use_past_key_values: KV cache Ïû¨ÏÇ¨Ïö© Ïó¨Î∂Ä (NoneÏù¥Î©¥ self.use_kv_cache ÏÇ¨Ïö©)
        """
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        text = text + "<|im_end|>"
        if self.nav_version == 'special_token':
            text = text.replace('<|vision_start|><|image_pad|><|vision_end|>', '')
            num_image = len(messages[0]['content']) - 1
            num_current_image = 3
            num_history_image = num_image - num_current_image

            history_img_str = ''.join(['<|vision_start|><|image_pad|><|vision_end|>'] * num_history_image)
            history_str_pos = text.rfind('Your historical pictures are: ') + len('Your historical pictures are: ')
            text = text[:history_str_pos] + history_img_str + text[history_str_pos:]

            text = text.replace('leftside: ', 'leftside: <|vision_start|><|image_pad|><|vision_end|>')
            text = text.replace('rightside: ', 'rightside: <|vision_start|><|image_pad|><|vision_end|>')
            text = text.replace('frontside: ', 'frontside: <|vision_start|><|image_pad|><|vision_end|>')
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(text=text, images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt")
        inputs = inputs.to("cuda")
        
        # Ï∞∏Í≥†: forward() Î©îÏÑúÎìúÎäî KV cacheÎ•º ÏßÅÏ†ë ÏßÄÏõêÌïòÏßÄ ÏïäÏùå
        # KV cacheÎäî generate() Î©îÏÑúÎìúÎÇò decoder_only Î™®Îç∏ÏóêÏÑúÎßå ÏôÑÏ†ÑÌûà ÏßÄÏõêÎê®
        # ÌòÑÏû¨ Íµ¨Ï°∞ÏóêÏÑúÎäî Í∞Å Ìò∏Ï∂úÎßàÎã§ Ï†ÑÏ≤¥ forward pass ÏàòÌñâ
        
        if flow_match == True:
            norm = [{"min": [
                         [-0.49142804741859436, -0.018926994875073433, -0.5000011853675626, 0.8660246981163404, 0.0],
                         [-0.8506758809089646, -0.11684392392635345, -0.5176391471000088, -0.36602701582911296, 0.0],
                         [-0.9391180276870728, -0.262770414352417, -0.5176390363234377, -0.5000015591363245, 0.0],
                         [-0.9319084137678146, -0.5872985124588013, -0.5176390363234377, -0.5176391890893195, 0.0],
                         [-0.9333658218383789, -0.8579317331314087, -0.5176390363233605, -0.5176391431200632, 0.0]],
                     "max": [[0.8222980499267578, 1.1485368013381958, 0.5000012222510074, 1.0, 1.0],
                             [0.8579317331314087, 1.0390557050704985, 0.5176391335634103, 0.13397477820902748, 1.0],
                             [0.9584183096885622, 0.9541159868240356, 0.5176391335632885, 0.3660255949191993, 1.0],
                             [0.9442337155342072, 0.9441415071487427, 0.5176391335631186, 0.5000004173778672, 1.0],
                             [0.9610724449157715, 0.9491362571716309, 0.5176391335630393, 0.5176390878671062, 1.0]]}]
            wp_pred, arrive_pred, sin_angle, cos_angle = self.model.forward(**inputs, norm=norm, action_former=True,
                                                                            gt_waypoints=0, train=False,
                                                                            train_branch=['continue'])
        else:
            # Î©îÎ™®Î¶¨ Ìö®Ïú®ÏÑ±ÏùÑ ÏúÑÌï¥ torch.no_grad() Î™ÖÏãúÏ†Å ÏÇ¨Ïö©
            with torch.no_grad():
                wp_pred, arrive_pred, sin_angle, cos_angle = self.model.forward(**inputs, action_former=True,
                                                                                gt_waypoints=0, train=False,
                                                                                train_branch=['continue'])
        # DEBUG: Print raw model output
        print(f"[DEBUG] wp_pred raw: {wp_pred.cpu().float().detach().numpy()}")
        print(f"[DEBUG] wp_pred min/max: {wp_pred.min().item():.6f} / {wp_pred.max().item():.6f}")
        # Free memory after inference - ÏûÖÎ†•Í≥º Ï§ëÍ∞Ñ ÌÖêÏÑú Ï†ïÎ¶¨
        del inputs
        torch.cuda.empty_cache()
        return wp_pred * PREDICT_SCALE, arrive_pred, sin_angle, cos_angle


class Waypoint_Agent():
    def __init__(self, model_path, result_path, require_map=True):

        print("Initialize Qwen")

        self.result_path = result_path
        self.require_map = require_map

        if not self.result_path is None:
            os.makedirs(self.result_path, exist_ok=True)
            os.makedirs(os.path.join(self.result_path, "log"), exist_ok=True)
            os.makedirs(os.path.join(self.result_path, "video"), exist_ok=True)
            os.makedirs(os.path.join(self.result_path, "map_vis"), exist_ok=True)
            os.makedirs(os.path.join(self.result_path, "render_img"), exist_ok=True)

        print("good")
        self.model = QwenModel(model_path)
        self.promt_template = "\n{instruction}"
        if flow_match:
            self.promt_template = """You are an autonomous navigation robot. You will get a task with historical pictures and current pictures you see.
Based on these information, you need to decide your next {num_action_trunck} actions, which could involve <|left|>,<|right|>,<|forward|>. If you finish your mission, output <|stop|>. Here are some examples: <|left|><|forward|><|forward|><|stop|>, <|forward|><|forward|><|forward|><|left|><|forward|> or <|stop|>
# Your historical pictures are: {history_img_string}
# {current_img_string}
# Your mission is: {instruction}<|NAV|>"""
        else:
            self.promt_template = """You are an autonomous navigation robot. You will get a task with historical pictures and current pictures you see.
Based on these information, you need to decide your next {num_action_trunck} actions, which could involve <|left|>,<|right|>,<|forward|>. If you finish your mission, output <|stop|>. Here are some examples: <|left|><|forward|><|forward|><|stop|>, <|forward|><|forward|><|forward|><|left|><|forward|> or <|stop|>
# Your historical pictures are: {history_img_string}
# {current_img_string}
# Your mission is: {instruction}<|NAV|>\nOutput the waypoint"""
        print("Initialization Complete")

        self.history_rgb_tensor = None
        self.rgb_list = []
        self.pose_list = []
        self.topdown_map_list = []
        self.count_id = 0
        self.reset()

    def addtext(self, image, instuction, navigation):
        h, w = image.shape[:2]
        new_height = h + 150
        new_image = np.zeros((new_height, w, 3), np.uint8)
        new_image.fill(255)
        new_image[:h, :w] = image

        font = cv2.FONT_HERSHEY_SIMPLEX
        textsize = cv2.getTextSize(instuction, font, 0.5, 2)[0]
        textY = h + (50 + textsize[1]) // 2

        y_line = textY + 0 * textsize[1]

        words = instuction.split(' ')
        max_width = new_image.shape[1]
        x = 10
        line = ""

        for word in words:

            test_line = line + ' ' + word if line else word
            test_line_size, _ = cv2.getTextSize(test_line, font, 0.5, 2)

            if test_line_size[0] > image.shape[1] - x:
                cv2.putText(new_image, line, (x, y_line), font, 0.5, (0, 0, 0), 2)
                line = word
                y_line += textsize[1] + 5
            else:
                line = test_line

        if line:
            cv2.putText(new_image, line, (x, y_line), font, 0.5, (0, 0, 0), 2)

        y_line = y_line + 1 * textsize[1] + 10
        new_image = cv2.putText(new_image, navigation, (x, y_line), font, 0.5, (0, 0, 0), 2)

        return new_image

    def reset(self):

        if self.require_map:
            if len(self.topdown_map_list) != 0:
                output_video_path = os.path.join(self.result_path, "video", "{}.gif".format(self.episode_id))
                
                if imageio is not None:
                    imageio.mimsave(output_video_path, self.topdown_map_list)
                else:
                    print(f"[WARN] imageio not available, skipping GIF save: {output_video_path}")

        self.history_rgb_tensor = None
        self.transformation_list = []
        self.rgb_list = []
        self.pose_list = []
        self.image_indices = []
        self.topdown_map_list = []
        self.total_frame_count = 0
        self.last_action = None
        self.count_id += 1
        self.count_stop = 0
        self.pending_action_list = []

        self.first_forward = False
        
        # KV cache Ï¥àÍ∏∞Ìôî
        if hasattr(self.model, 'past_key_values'):
            self.model.past_key_values = None

    def pose_to_matrix(self, pose):
        ## È´òÊñØ‰ªøÁúüÂùêÊ†áÁ≥ª ÂâçÂùê‰∏ä
        if isinstance(pose, np.ndarray):
            rotation_matrix = pose[:3, :3]
            position = pose[:3, 3]
            rot_normal_raw = np.array([[0, 0, 1], [-1, 0, 0], [0, -1, 0]])
            rotation_matrix = rotation_matrix @ rot_normal_raw
            pose = np.eye(4)
            pose[:3, :3] = rotation_matrix
            pose[:3, 3] = position

        else:  ## habitat‰ªøÁúüÂùêÊ†áÁ≥ª
            position = np.array(pose['position'])
            rotation = np.array(pose['rotation'])

            rotation_matrix = R.from_quat(rotation[[1, 2, 3, 0]]).as_matrix()

            rot_normal_raw = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]])
            rotation_matrix = rotation_matrix @ rot_normal_raw
            pose = np.eye(4)
            pose[:3, :3] = rotation_matrix
            pose[:3, 3] = position
        return pose

    def transform_poses_to_local(self, current_pose, input_poses):
        current_pose = self.pose_to_matrix(current_pose)
        current_pose_inv = np.linalg.inv(current_pose)

        output_poses = [current_pose_inv @ self.pose_to_matrix(pose) for pose in input_poses]
        return output_poses

    def generate_infer_prompt(self, instruction):
        cur_prompt = deepcopy(self.promt_template)

        input_poses = deepcopy(self.pose_list)
        local_poses = self.transform_poses_to_local(self.pose_list[-1], input_poses)

        input_positions = [[pose[0, 3], pose[2, 3]] for pose in local_poses]
        images = self.rgb_list

        history_pose_strings = ['<{:.3f},{:.3f}>'.format(pose[0], pose[1]) for pose in input_positions]
        history_pose_string = ",".join(history_pose_strings)

        history_img_string = ''
        # current_img_string = "Your current observations is leftside: , frontside: , rightside: "
        current_img_string = "Your current observations is leftside: , rightside: , frontside: "

        cur_prompt = cur_prompt.format(instruction=instruction, history_pose_string=history_pose_string,
                                       step_scale=PREDICT_SCALE, num_action_trunck=NUM_ACTION_TRUNK,
                                       current_img_string=current_img_string, history_img_string=history_img_string)

        return self.model.qwen_data_pack(images, cur_prompt)

    def add_frame(self, rgbs, pose):
        """
        Ê∑ªÂä†Êñ∞ÁöÑRGBÂõæÂÉèÂíåposeÔºåÂÆûÁé∞Êô∫ËÉΩÁöÑÂéÜÂè≤Â∏ßÁÆ°ÁêÜ
        1. ÈôêÂà∂ÊúÄÂ§ßÂõæÁâáÊï∞Èáè‰∏∫MAX_HISTORY_FRAMES
        2. ‰øùËØÅÁ¨¨‰∏ÄÂº†ÂõæÂÉè‰∏ÄÁõ¥Âú®ÈòüÂàó‰∏≠
        3. Ë∂ÖËøáMAX_HISTORY_FRAMESÊó∂ÔºåËøõË°åÁ±ª‰ººÂùáÂåÄÈááÊ†∑ÁöÑÈáçÊñ∞ÈááÊ†∑
        """

        # TODO: poseÈúÄË¶ÅÊåâÁÖßÂ§ñÂèÇÁÆ°ÁêÜ
        # ËÆ°Êó∂
        start_time = time.time()
        # Â∞ÜÊñ∞ÁöÑrgbÂíåposeÊ∑ªÂä†Âà∞ÂÆåÊï¥ÂàóË°®‰∏≠
        rgbs_new = []
        for rgb in rgbs:
            if isinstance(rgb, np.ndarray):
                rgb_img = Image.fromarray(rgb)
                # Áªü‰∏ÄË∞ÉÊï¥Â§ßÂ∞èÂà∞Ê®°ÂûãËæìÂÖ•Â∞∫ÂØ∏
                rgb = rgb_img.resize((INPUT_IMG_SIZE[0], INPUT_IMG_SIZE[1]))
            else:
                import sys
                sys.exit(0)
                rgb = rgb.resize((INPUT_IMG_SIZE[0], INPUT_IMG_SIZE[1]))
            rgbs_new.append(rgb)

        if len(self.rgb_list) >= NUM_CURRENT_IMAGE:
            # ÂéÜÂè≤Â∏ßÂè™‰øùÁïôfront
            for _ in range(NUM_CURRENT_IMAGE - 1):
                self.rgb_list.pop(-2)
                self.pose_list.pop(-2)
                self.image_indices.pop(-2)
            # pop_idx = [-1, -2]
            # for idx in pop_idx:
            #     self.rgb_list.pop(idx)
            #     self.pose_list.pop(idx)
            #     self.image_indices.pop(idx)

        self.rgb_list.extend(rgbs_new)
        self.pose_list.extend([pose] * len(rgbs_new))
        self.image_indices.extend([self.total_frame_count] * len(rgbs_new))
        self.total_frame_count += 1
        if len(self.rgb_list) > NUM_CURRENT_IMAGE:
            self.rgb_list[-1 - NUM_CURRENT_IMAGE] = self.rgb_list[-1 - NUM_CURRENT_IMAGE].resize(
                (int(INPUT_IMG_SIZE[0] * HISTORY_RESIZE_RATIO), int(INPUT_IMG_SIZE[1] * HISTORY_RESIZE_RATIO)))

        # Â¶ÇÊûúË∂ÖËøáÊúÄÂ§ßÂéÜÂè≤Â∏ßÊï∞ÔºåÈúÄË¶ÅÈáçÊñ∞ÈááÊ†∑
        if len(self.rgb_list) > MAX_HISTORY_FRAMES + NUM_CURRENT_IMAGE:
            # Âü∫‰∫éself.image_indices ÁßªÈô§Á¨¨‰∏Ä‰∏™Èó¥Ë∑ùÊúÄÂ∞èÁöÑÂ∏ß
            min_interval_idx = np.argmin(np.diff(self.image_indices[:-NUM_CURRENT_IMAGE]))
            self.rgb_list.pop(min_interval_idx + 1)
            self.pose_list.pop(min_interval_idx + 1)
            self.image_indices.pop(min_interval_idx + 1)

        print('current image_indices: {}'.format(self.image_indices))
        end_time = time.time()
        print(f"add_frame elapsed: {end_time - start_time:.4f}s")

    def act(self, observations, info, episode_id):

        self.episode_id = episode_id
        cur_episode_folder = os.path.join(self.result_path, "render_img", str(episode_id))
        os.makedirs(cur_episode_folder, exist_ok=True)

        cur_episode_vis_folder = os.path.join(self.result_path, "map_vis", str(episode_id))
        os.makedirs(cur_episode_vis_folder, exist_ok=True)

        if self.model.nav_version == 'special_token':
            rgb = observations["front"]
            pose = observations["pose"]
        else:
            print("??????????")
            rgb = observations["rgb"]
            pose = observations["pose"]

        # ‰øùÂ≠òÂΩìÂâçÂõæÂÉèÔºà‰ΩøÁî®Âç≥Â∞ÜÂàÜÈÖçÁöÑÂéüÂßãÂ∏ßÁ¥¢ÂºïÔºâ
        current_frame_index = self.total_frame_count
        output_img_path = os.path.join(cur_episode_folder, "{}.png".format(current_frame_index))
        if SAVE_RANDER_IMG:
            cv2.imwrite(output_img_path, cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))

        # ‰ΩøÁî®add_frameÊñπÊ≥ïÊ∑ªÂä†Êñ∞ÁöÑÂõæÂÉèÂíåpose
        if self.model.nav_version == 'special_token':
            self.add_frame([observations['left'], observations['right'], observations['front']], pose)
            # self.add_frame([observations['left'], observations['front'], observations['right']], pose)
        else:
            self.add_frame([rgb], pose)

        if self.require_map:
            # RGB Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï§ÑÏù¥Í∏∞ (ÎÑàÎ¨¥ ÌÅ¨Î©¥ Î∞ëÏóê Í∏ÄÏûêÎÇò Ïò§Î•∏Ï™Ω pathÍ∞Ä Ïûò Ïïà Î≥¥ÏûÑ)
            # ÏõêÎ≥∏ ÌÅ¨Í∏∞Î≥¥Îã§ ÏûëÍ≤å Î¶¨ÏÇ¨Ïù¥Ï¶à (Ïòà: ÎÜíÏù¥ 400)
            rgb_resize_height = 400
            if rgb.shape[0] > rgb_resize_height:
                aspect_ratio = rgb.shape[1] / rgb.shape[0]
                rgb_resize_width = int(rgb_resize_height * aspect_ratio)
                rgb_small = cv2.resize(rgb, (rgb_resize_width, rgb_resize_height), interpolation=cv2.INTER_LINEAR)
            else:
                rgb_small = rgb
            
            # GT ÎßµÍ≥º ÏòàÏ∏° ÎßµÏùÑ Î≥ÑÎèÑÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞
            gt_map_raw = info.get("gt_map", np.zeros((256, 256), dtype=np.uint8))
            pred_map_raw = info.get("pred_map", np.zeros((256, 256), dtype=np.uint8))
            
            # Îßµ Ïª¨Îü¨ Î≥ÄÌôò (habitat ÏóÜÏù¥ ÏßÅÏ†ë Ï≤òÎ¶¨)
            TOP_DOWN_MAP_COLORS = np.full((256, 3), 150, dtype=np.uint8)  # Í∏∞Î≥∏ ÌöåÏÉâ
            TOP_DOWN_MAP_COLORS[0] = [255, 255, 255]  # MAP_INVALID_POINT = 0 = Ìù∞ÏÉâ
            TOP_DOWN_MAP_COLORS[1] = [150, 150, 150]  # MAP_VALID_POINT = 1 = ÌöåÏÉâ
            TOP_DOWN_MAP_COLORS[10] = [0, 0, 0]       # MAP_REFERENCE_POINT = 10 = Í≤ÄÏùÄÏÉâ (GT)
            TOP_DOWN_MAP_COLORS[12] = [255, 255, 0]   # MAP_WAYPOINT_PREDICTION = 12 = ÎÖ∏ÎûÄÏÉâ (ÏòàÏ∏°)
            
            # JET colormap Ï†ÅÏö© (15~255)
            if len(gt_map_raw.shape) == 2:
                jet_colors = cv2.applyColorMap(np.arange(241, dtype=np.uint8), cv2.COLORMAP_JET).squeeze(1)
                TOP_DOWN_MAP_COLORS[15:] = jet_colors[:, ::-1]  # BGR -> RGB
            
            # GT Îßµ Ïª¨Îü¨ Î≥ÄÌôò
            if len(gt_map_raw.shape) == 2:
                gt_map = TOP_DOWN_MAP_COLORS[gt_map_raw]
            else:
                gt_map = gt_map_raw
            
            # ÏòàÏ∏° Îßµ Ïª¨Îü¨ Î≥ÄÌôò
            if len(pred_map_raw.shape) == 2:
                pred_map = TOP_DOWN_MAP_COLORS[pred_map_raw]
            else:
                pred_map = pred_map_raw
            
            # Îßµ ÌÅ¨Í∏∞ ÎßûÏ∂îÍ∏∞ (Í∞ÄÎ°ú ÌÅ¨Í∏∞Îäî RGBÏôÄ ÎßûÏ∂îÍ≥†, ÎÜíÏù¥Îäî Ï†ÅÏ†àÌûà)
            map_width = rgb_small.shape[1]
            
            # GT Îßµ Î¶¨ÏÇ¨Ïù¥Ï¶à
            gt_h, gt_w = gt_map.shape[:2]
            if gt_w > 0:
                gt_aspect = gt_h / gt_w
                gt_target_h = int(map_width * gt_aspect)
                gt_map_resized = cv2.resize(gt_map, (map_width, gt_target_h), interpolation=cv2.INTER_CUBIC)
            else:
                gt_map_resized = gt_map
            
            # ÏòàÏ∏° Îßµ Î¶¨ÏÇ¨Ïù¥Ï¶à
            pred_h, pred_w = pred_map.shape[:2]
            if pred_w > 0:
                pred_aspect = pred_h / pred_w
                pred_target_h = int(map_width * pred_aspect)
                pred_map_resized = cv2.resize(pred_map, (map_width, pred_target_h), interpolation=cv2.INTER_CUBIC)
            else:
                pred_map_resized = pred_map
            
            # RGB ÏúÑÏóê GT Îßµ, ÏòàÏ∏° ÎßµÏùÑ ÏÑ∏Î°úÎ°ú Î∞∞Ïπò
            # RGB (ÏûëÍ≤å) | GT Îßµ
            #            | ÏòàÏ∏° Îßµ
            # Î®ºÏ†Ä GT ÎßµÍ≥º ÏòàÏ∏° ÎßµÏùÑ ÏÑ∏Î°úÎ°ú Ìï©ÏπòÍ∏∞
            combined_map_h = gt_map_resized.shape[0] + pred_map_resized.shape[0]
            combined_map = np.zeros((combined_map_h, map_width, 3), dtype=np.uint8)
            combined_map[:gt_map_resized.shape[0], :] = gt_map_resized
            combined_map[gt_map_resized.shape[0]:, :] = pred_map_resized
            
            # RGB ÎÜíÏù¥Ïóê ÎßûÏ∂∞ combined_map Î¶¨ÏÇ¨Ïù¥Ï¶à
            target_map_height = rgb_small.shape[0]
            combined_map_final = cv2.resize(combined_map, (map_width, target_map_height), interpolation=cv2.INTER_CUBIC)
            
            # RGBÏôÄ ÎßµÏùÑ Í∞ÄÎ°úÎ°ú Ìï©ÏπòÍ∏∞
            output_im = np.concatenate((rgb_small, combined_map_final), axis=1)

        if len(self.pending_action_list) != 0:
            temp_action = self.pending_action_list.pop(0)

            if self.require_map:
                img = self.addtext(output_im, observations["instruction"]["text"],
                                   "Pending action: {}".format(temp_action))
                self.topdown_map_list.append(img)
                cv2.imwrite(os.path.join(cur_episode_vis_folder, "{}.png".format(current_frame_index)),
                            cv2.cvtColor(img, cv2.COLOR_RGB2BGR))

            return {"action": temp_action}

        start_time = time.time()
        navigation_qs = self.generate_infer_prompt(observations["instruction"]["text"])
        end_time = time.time()
        print(f"generate_infer_prompt elapsed: {end_time - start_time:.4f}s")

        start_time = time.time()
        # print("question")
        # print(navigation_qs)
        wp_pred_, src_arrive_pred, sin_angle, cos_angle = self.model.qwen_infer(navigation_qs)
        end_time = time.time()
        print(f"qwen_infer elapsed: {end_time - start_time:.4f}s")
        if flow_match:
            print(src_arrive_pred.squeeze())
            cnt = 0
            for cur_arrive in src_arrive_pred.squeeze():
                if cur_arrive.item() > 0.5:
                    cnt += 1
            print(cnt)
            if cnt == 5:
                arrive_pred = 1
            else:
                arrive_pred = 0
        else:
            cnt = 0
            for cur_arrive in src_arrive_pred.squeeze():
                if cur_arrive.item() >= 0:
                    cnt += 1
            # print(cnt)
            if cnt == 5:
                arrive_pred = 1
            else:
                arrive_pred = 0
            # arrive_pred = arrive_pred.item()
            # arrive_pred = arrive_pred.cpu().type(torch.float32).numpy().squeeze()
        wp_pred_ = wp_pred_.cpu().type(torch.float32).numpy().squeeze()
        recover_angle = torch.atan2(sin_angle, cos_angle).detach().cpu().type(torch.float32).numpy().squeeze()
        sin_angle_np = sin_angle.detach().cpu().type(torch.float32).numpy().squeeze()
        cos_angle_np = cos_angle.detach().cpu().type(torch.float32).numpy().squeeze()

        if self.require_map:
            img = self.addtext(output_im, observations["instruction"]["text"], '{},arrive_pred_{},{},{}'.format(cnt,
                                                                                                                src_arrive_pred.detach().cpu().type(
                                                                                                                    torch.float32).numpy().squeeze(),
                                                                                                                wp_pred_[
                                                                                                                    0],
                                                                                                                recover_angle))
            # img = output_im
            self.topdown_map_list.append(img)
            cv2.imwrite(os.path.join(cur_episode_vis_folder, "{}.png".format(current_frame_index)),
                        cv2.cvtColor(img, cv2.COLOR_RGB2BGR))

        return {"action": wp_pred_, "arrive_pred": arrive_pred, "recover_angle": recover_angle, 
                "sin_angle": sin_angle_np, "cos_angle": cos_angle_np}

